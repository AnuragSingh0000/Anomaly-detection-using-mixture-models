{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from train import TrainerDAGMM\n",
    "from preprocess import get_KDDCup99\n",
    "\n",
    "\n",
    "def eval(model, dataloaders, device, n_gmm, sub = 30):\n",
    "    \"\"\"Testing the DAGMM model with added reconstruction loss\"\"\"\n",
    "    dataloader_train, dataloader_test = dataloaders\n",
    "    model.eval()\n",
    "    print('Testing...')\n",
    "    compute = ComputeLoss(model, None, None, device, n_gmm)\n",
    "    with torch.no_grad():\n",
    "        N_samples = 0\n",
    "        gamma_sum = 0\n",
    "        mu_sum = 0\n",
    "        cov_sum = 0\n",
    "        # Obtaining the parameters gamma, mu and cov using the training (clean) data.\n",
    "        for x, _ in dataloader_train:\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _, _, z, gamma = model(x)\n",
    "            phi_batch, mu_batch, cov_batch = compute.compute_params(z, gamma)\n",
    "\n",
    "            batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum += mu_batch * batch_gamma_sum.unsqueeze(-1)\n",
    "            cov_sum += cov_batch * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            N_samples += x.size(0)\n",
    "\n",
    "        train_phi = gamma_sum / N_samples\n",
    "        train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n",
    "        train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Obtaining Labels and energy scores for train data\n",
    "        energy_train = []\n",
    "        labels_train = []\n",
    "        for x, y in dataloader_train:\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _, _, z, gamma = model(x)\n",
    "            sample_energy, cov_diag  = compute.compute_energy(z, gamma, phi=train_phi,\n",
    "                                                              mu=train_mu, cov=train_cov,\n",
    "                                                              sample_mean=False)\n",
    "            # Add reconstruction loss\n",
    "            r = model.decode(model.encode(x))\n",
    "            recon_loss = torch.norm(x - r, p=2, dim=1)\n",
    "            sample_energy = sample_energy + recon_loss\n",
    "\n",
    "            energy_train.append(sample_energy.detach().cpu())\n",
    "            labels_train.append(y)\n",
    "        energy_train = torch.cat(energy_train).numpy()\n",
    "        labels_train = torch.cat(labels_train).numpy()\n",
    "\n",
    "        # Obtaining Labels and energy scores for test data\n",
    "        energy_test = []\n",
    "        labels_test = []\n",
    "        for x, y in dataloader_test:\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _, _, z, gamma = model(x)\n",
    "            sample_energy, cov_diag  = compute.compute_energy(z, gamma, train_phi,\n",
    "                                                              train_mu, train_cov,\n",
    "                                                              sample_mean=False)\n",
    "            # Add reconstruction loss\n",
    "            r = model.decode(model.encode(x))\n",
    "            recon_loss = torch.norm(x - r, p=2, dim=1)\n",
    "            sample_energy = sample_energy + recon_loss\n",
    "\n",
    "            energy_test.append(sample_energy.detach().cpu())\n",
    "            labels_test.append(y)\n",
    "        energy_test = torch.cat(energy_test).numpy()\n",
    "        labels_test = torch.cat(labels_test).numpy()\n",
    "\n",
    "        scores_total = np.concatenate((energy_train, energy_test), axis=0)\n",
    "        labels_total = np.concatenate((labels_train, labels_test), axis=0)\n",
    "\n",
    "    threshold = np.percentile(scores_total, 100 - sub)\n",
    "    pred = (energy_test > threshold).astype(int)\n",
    "    gt = labels_test.astype(int)\n",
    "    precision, recall, f_score, _ = prf(gt, pred, average='binary')\n",
    "    print(\"Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(precision, recall, f_score))\n",
    "    print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels_total, scores_total)*100))\n",
    "    return labels_total, scores_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from train import TrainerDAGMM\n",
    "# from test import eval\n",
    "from preprocess import get_KDDCup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 0, Loss: 41633791.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 1, Loss: 41566614.433\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 2, Loss: 41596714.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 3, Loss: 41650338.825\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 4, Loss: 41675119.557\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 5, Loss: 41640314.619\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 6, Loss: 41541884.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 7, Loss: 41656896.485\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 8, Loss: 41635662.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 9, Loss: 41668599.784\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 10, Loss: 41773137.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 11, Loss: 41540296.320\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 12, Loss: 41743364.103\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 13, Loss: 41651250.742\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 14, Loss: 41832839.361\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 15, Loss: 41588881.309\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 16, Loss: 41606761.443\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 17, Loss: 41634653.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 18, Loss: 41829801.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 19, Loss: 41753661.856\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 20, Loss: 41625575.340\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 21, Loss: 41515833.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 22, Loss: 41484060.918\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 23, Loss: 41607224.515\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 24, Loss: 41560909.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 25, Loss: 41442437.567\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 26, Loss: 41600993.794\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 27, Loss: 41779583.763\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 28, Loss: 41744978.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 29, Loss: 41755418.155\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 30, Loss: 41578518.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 31, Loss: 41838460.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 32, Loss: 41512113.247\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 33, Loss: 41556589.278\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 34, Loss: 41618695.742\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 35, Loss: 41581626.649\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 36, Loss: 41646465.320\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 37, Loss: 41614099.175\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 38, Loss: 41488652.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 39, Loss: 41594007.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 40, Loss: 41686233.505\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 41, Loss: 41581310.289\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 42, Loss: 41681351.876\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 43, Loss: 41727015.268\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 44, Loss: 41632001.505\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 45, Loss: 41578188.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 46, Loss: 41536910.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 47, Loss: 41712815.196\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 48, Loss: 41524580.804\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 49, Loss: 41873404.557\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 50, Loss: 41697945.866\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 51, Loss: 41832192.753\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 52, Loss: 41500419.000\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 53, Loss: 41606576.144\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 54, Loss: 41681920.722\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 55, Loss: 41643796.155\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 56, Loss: 41563108.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 57, Loss: 41570752.990\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 58, Loss: 41679494.052\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 59, Loss: 41710378.392\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 60, Loss: 41781326.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 61, Loss: 41634718.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 62, Loss: 41574619.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 63, Loss: 41606480.598\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 64, Loss: 41600093.206\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 65, Loss: 41595202.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 66, Loss: 41560873.742\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 67, Loss: 41837431.320\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 68, Loss: 41774298.134\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 69, Loss: 41557733.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 70, Loss: 41560432.155\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 71, Loss: 41673708.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 72, Loss: 41859798.392\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 73, Loss: 41584393.206\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 74, Loss: 41720593.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 75, Loss: 41603094.567\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 76, Loss: 41726941.268\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 77, Loss: 41738951.206\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 78, Loss: 41773102.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 79, Loss: 41507125.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 80, Loss: 41734753.711\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 81, Loss: 41725332.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 82, Loss: 41719598.258\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 83, Loss: 41629758.691\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 84, Loss: 41706791.649\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 85, Loss: 41658911.216\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 86, Loss: 41560607.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 87, Loss: 41572728.567\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 88, Loss: 41497800.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 89, Loss: 41658730.000\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 90, Loss: 41584710.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 91, Loss: 41666555.454\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 92, Loss: 41853052.423\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 93, Loss: 41601237.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 94, Loss: 41596187.474\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 95, Loss: 41674962.773\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 96, Loss: 41778304.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 97, Loss: 41604700.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 98, Loss: 41547084.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 99, Loss: 41506471.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 100, Loss: 41616046.670\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 101, Loss: 41648259.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 102, Loss: 41724476.351\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 103, Loss: 41594622.505\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 104, Loss: 41596151.825\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 105, Loss: 41607095.825\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 106, Loss: 41662675.361\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 107, Loss: 41630083.526\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 108, Loss: 41650351.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 109, Loss: 41526296.670\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 110, Loss: 41711463.505\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 111, Loss: 41614943.866\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 112, Loss: 41714659.959\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 113, Loss: 41701344.371\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 114, Loss: 41701761.423\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 115, Loss: 41510219.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 116, Loss: 41674282.206\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 117, Loss: 41640379.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 118, Loss: 41559052.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 119, Loss: 41645401.093\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 120, Loss: 41713886.144\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 121, Loss: 41618357.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 122, Loss: 41590740.289\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 123, Loss: 41582567.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 124, Loss: 41692132.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 125, Loss: 41503342.866\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 126, Loss: 41585618.619\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 127, Loss: 41838130.412\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 128, Loss: 41566934.093\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 129, Loss: 41524339.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 130, Loss: 41572781.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 131, Loss: 41791113.361\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 132, Loss: 41548789.619\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 133, Loss: 41609049.784\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 134, Loss: 41643529.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 135, Loss: 41603381.021\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 136, Loss: 41428681.000\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 137, Loss: 41589177.258\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 138, Loss: 41760217.866\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 139, Loss: 41475812.021\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 140, Loss: 41587992.722\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 141, Loss: 41695286.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 142, Loss: 41694005.216\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 143, Loss: 41624143.361\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 144, Loss: 41671185.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 145, Loss: 41477904.784\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 146, Loss: 41715900.340\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 147, Loss: 41626048.722\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 148, Loss: 41609715.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 149, Loss: 41797336.268\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 150, Loss: 41730462.515\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 151, Loss: 41842037.340\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 152, Loss: 41751322.485\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 153, Loss: 41757370.361\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 154, Loss: 41667174.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 155, Loss: 41621228.309\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 156, Loss: 41708515.082\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 157, Loss: 41863285.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 158, Loss: 41796504.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 159, Loss: 41686898.485\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 160, Loss: 41682621.557\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 161, Loss: 41557974.918\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 162, Loss: 41680778.278\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 163, Loss: 41477981.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 164, Loss: 41616491.196\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 165, Loss: 41766079.856\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 166, Loss: 41694806.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 167, Loss: 41582588.351\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 168, Loss: 41519038.722\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 169, Loss: 41472508.216\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 170, Loss: 41696776.660\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 171, Loss: 41841042.876\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 172, Loss: 41621614.299\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 173, Loss: 41764442.485\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 174, Loss: 41593502.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 175, Loss: 41679574.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 176, Loss: 41611956.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 177, Loss: 41724045.041\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 178, Loss: 41640236.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 179, Loss: 41506699.825\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 180, Loss: 41619305.021\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 181, Loss: 41758531.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 182, Loss: 41711302.423\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 183, Loss: 41560409.433\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 184, Loss: 41564953.897\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 185, Loss: 41806950.423\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 186, Loss: 41537039.557\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 187, Loss: 41815459.485\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 188, Loss: 41662891.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 189, Loss: 41677760.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 190, Loss: 41622697.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 191, Loss: 41749023.021\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 192, Loss: 41586865.990\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 193, Loss: 41613735.794\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 194, Loss: 41647732.763\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 195, Loss: 41753995.412\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 196, Loss: 41729194.670\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 197, Loss: 41610367.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 198, Loss: 41513293.144\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 199, Loss: 41663114.268\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=200\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=1024\n",
    "    latent_dim=1\n",
    "    n_gmm=4\n",
    "    lambda_energy=0.1\n",
    "    lambda_cov=0.005\n",
    "\n",
    "\n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_KDDCup99(args)\n",
    "\n",
    "dagmm = TrainerDAGMM(args, data, device)\n",
    "dagmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.9464, Recall : 0.4557, F-score : 0.6152\n",
      "ROC AUC score: 98.62\n",
      "20\n",
      "Testing...\n",
      "Precision : 0.9505, Recall : 0.9200, F-score : 0.9350\n",
      "ROC AUC score: 98.62\n",
      "30\n",
      "Testing...\n",
      "Precision : 0.7919, Recall : 0.9995, F-score : 0.8837\n",
      "ROC AUC score: 98.62\n",
      "40\n",
      "Testing...\n",
      "Precision : 0.6590, Recall : 0.9996, F-score : 0.7943\n",
      "ROC AUC score: 98.62\n",
      "50\n",
      "Testing...\n",
      "Precision : 0.5952, Recall : 0.9999, F-score : 0.7462\n",
      "ROC AUC score: 98.62\n",
      "60\n",
      "Testing...\n",
      "Precision : 0.5952, Recall : 0.9999, F-score : 0.7462\n",
      "ROC AUC score: 98.62\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n",
    "\n",
    "from forward_step import ComputeLoss\n",
    "\n",
    "sub = [10,20,30, 40, 50, 60]\n",
    "for s in sub:\n",
    "    print(s)\n",
    "    labels, scores = eval(dagmm.model, data, device, args.n_gmm, sub=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_in = scores[np.where(labels==0)[0]]\n",
    "scores_out = scores[np.where(labels==1)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
