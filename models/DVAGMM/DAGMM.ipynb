{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "\n",
    "from forward_step import ComputeLossVAE\n",
    "from model import DAGMM_VAE\n",
    "\n",
    "def eval(args, model, dataloaders, device, sub=20):\n",
    "    \"\"\"Evaluate the DAGMM-VAE model with GMM energy scoring.\"\"\"\n",
    "    train_loader, test_loader = dataloaders\n",
    "    model.eval()\n",
    "    print('Evaluating DAGMM-VAE...')\n",
    "\n",
    "    # Use ComputeLossVAE only for its compute_params and compute_energy methods\n",
    "    compute = ComputeLossVAE(\n",
    "        lambda_energy=args.lambda_energy,\n",
    "        lambda_cov=args.lambda_cov,\n",
    "        lambda_kl=args.lambda_kl,\n",
    "        device=device,\n",
    "        n_gmm=args.n_gmm\n",
    "    )\n",
    "\n",
    "    # 1) Estimate GMM parameters on training (clean) data\n",
    "    with torch.no_grad():\n",
    "        N = 0\n",
    "        gamma_sum = 0\n",
    "        mu_sum = 0\n",
    "        cov_sum = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.float().to(device)\n",
    "            outputs = model(x)\n",
    "            z = outputs['z']\n",
    "            gamma = outputs['gamma']\n",
    "\n",
    "            phi_batch, mu_batch, cov_batch = compute.compute_params(z, gamma)\n",
    "            batch_gamma_sum = gamma.sum(dim=0)\n",
    "\n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum    += mu_batch * batch_gamma_sum.unsqueeze(-1)\n",
    "            cov_sum   += cov_batch * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "            N += x.size(0)\n",
    "\n",
    "        phi = gamma_sum / N\n",
    "        mu  = mu_sum / gamma_sum.unsqueeze(-1)\n",
    "        cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    # 2) Compute energy scores for train and test\n",
    "    def get_scores(loader):\n",
    "        scores, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.float().to(device)\n",
    "                outputs = model(x)\n",
    "                z = outputs['z']\n",
    "                gamma = outputs['gamma']\n",
    "                energy, _ = compute.compute_energy(z, gamma, phi=phi, mu=mu, cov=cov, sample_mean=False)\n",
    "                scores.append(energy.cpu())\n",
    "                labels.append(y)\n",
    "        return torch.cat(scores).numpy(), torch.cat(labels).numpy()\n",
    "\n",
    "    energy_train, labels_train = get_scores(train_loader)\n",
    "    energy_test,  labels_test  = get_scores(test_loader)\n",
    "\n",
    "    # Combine for threshold and AUC\n",
    "    all_scores = np.concatenate([energy_train, energy_test])\n",
    "    all_labels = np.concatenate([labels_train,  labels_test])\n",
    "\n",
    "    # Set threshold (e.g., top 20% anomalies)\n",
    "    thresh = np.percentile(all_scores, 100-sub)\n",
    "    preds = (energy_test > thresh).astype(int)\n",
    "\n",
    "    precision, recall, f1, _ = prf(labels_test, preds, average='binary')\n",
    "    auc = roc_auc_score(all_labels, all_scores)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {auc*100:.2f}%\")\n",
    "\n",
    "    return all_labels, all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from preprocess import get_KDDCup99\n",
    "from train import TrainerDAGMMVAE\n",
    "# from test import eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 0, Loss: 0.188\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 1, Loss: 0.153\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 2, Loss: 0.142\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 3, Loss: 0.141\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 4, Loss: 0.136\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 5, Loss: 0.120\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 6, Loss: 0.112\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 7, Loss: 0.109\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 8, Loss: 0.107\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 9, Loss: 0.104\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 10, Loss: 0.100\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 11, Loss: 0.096\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 12, Loss: 0.093\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 13, Loss: 0.090\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 14, Loss: 0.088\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 15, Loss: 0.087\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 16, Loss: 0.086\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 17, Loss: 0.085\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 18, Loss: 0.085\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 19, Loss: 0.084\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 20, Loss: 0.084\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 21, Loss: 0.083\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 22, Loss: 0.083\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 23, Loss: 0.082\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 24, Loss: 0.082\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 25, Loss: 0.081\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 26, Loss: 0.080\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 27, Loss: 0.080\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 28, Loss: 0.079\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 29, Loss: 0.078\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM-VAE... Epoch: 30, Loss: 0.078\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 31, Loss: 0.078\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM-VAE... Epoch: 32, Loss: 0.077\n",
      "196608/198371: [===============================>] - ETA 0.2ss"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 1) Train\u001b[39;00m\n\u001b[1;32m     18\u001b[0m vae_trainer \u001b[38;5;241m=\u001b[39m TrainerDAGMMVAE(args, data, device)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mvae_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DAGMM/DVAGMM/train.py:37\u001b[0m, in \u001b[0;36mTrainerDAGMMVAE.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m     36\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, _ \u001b[38;5;129;01min\u001b[39;00m Bar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[1;32m     38\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     39\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/barbar/Bar.py:35\u001b[0m, in \u001b[0;36mBar.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:285\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "File \u001b[0;32m~/miniconda/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:285\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class Args:\n",
    "    num_epochs     = 100\n",
    "    patience       = 50\n",
    "    lr             = 1e-4\n",
    "    lr_milestones  = [50]\n",
    "    batch_size     = 1024\n",
    "    latent_dim     = 1\n",
    "    n_gmm          = 4\n",
    "    lambda_energy  = 0.1\n",
    "    lambda_cov     = 0.005\n",
    "    lambda_kl      = 0.0\n",
    "\n",
    "args   = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data   = get_KDDCup99(args)\n",
    "vae_trainer = TrainerDAGMMVAE(args, data, device)\n",
    "vae_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Threshold: top 10% anomalies ===\n",
      "Evaluating DAGMM-VAE...\n",
      "Precision: 0.2387, Recall: 0.0688, F1: 0.1069\n",
      "ROC AUC: 47.90%\n",
      "\n",
      "=== Threshold: top 20% anomalies ===\n",
      "Evaluating DAGMM-VAE...\n",
      "Precision: 0.2898, Recall: 0.1733, F1: 0.2169\n",
      "ROC AUC: 47.91%\n",
      "\n",
      "=== Threshold: top 30% anomalies ===\n",
      "Evaluating DAGMM-VAE...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Evaluate at multiple thresholds\n",
    "sub_percentiles = [10, 20, 30, 40, 50, 60]\n",
    "for sub in sub_percentiles:\n",
    "    print(f\"\\n=== Threshold: top {sub}% anomalies ===\")\n",
    "    y_true, scores = eval(args, vae_trainer.model, data, device, sub)\n",
    "    # if you want to plot the score distribution:\n",
    "    # plt.hist(scores, bins=50); plt.title(f\"Score hist at {sub}%\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_inlier_outlier_kde(labels, scores, model_name='DAGMM', sub=None):\n",
    "    \"\"\"\n",
    "    Plot KDEs of inlier (label=0) vs outlier (label=1) score distributions.\n",
    "\n",
    "    Args:\n",
    "        labels (array-like): 0 for inliers, 1 for outliers\n",
    "        scores (array-like): anomaly scores\n",
    "        model_name (str): Used in the title\n",
    "        sub (int, optional): percentile threshold used, if any\n",
    "    \"\"\"\n",
    "    # Split scores\n",
    "    scores_in  = scores[np.where(labels == 0)[0]]\n",
    "    scores_out = scores[np.where(labels == 1)[0]]\n",
    "\n",
    "    # Make DataFrames\n",
    "    df_in  = pd.DataFrame(scores_in,  columns=['Inlier'])\n",
    "    df_out = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    df_in .plot.kde(ax=ax, legend=True)\n",
    "    df_out.plot.kde(ax=ax, legend=True)\n",
    "\n",
    "    # Title & grids\n",
    "    title = f'{model_name} Inlier vs Outlier KDE'\n",
    "    if sub is not None:\n",
    "        title += f' (top {sub}% threshold)'\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Anomaly Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_vae, scores_vae = eval(vae_trainer.model, data, device, args.n_gmm, sub=30)\n",
    "plot_inlier_outlier_kde(labels_vae, scores_vae, model_name='DAGMMâ€‘VAE', sub=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
